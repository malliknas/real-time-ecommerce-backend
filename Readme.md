
---

# Real-Time E-commerce Backend

This project implements a **real-time backend architecture** for processing and visualizing synthetic e-commerce transaction data using a fully containerized microservices approach. The system is designed to simulate, stream, store, and visualize transaction flows in real-time using technologies like **Apache Kafka**, **PostgreSQL**, **FastAPI**, and **Streamlit**, all orchestrated through **Docker Compose**.

The goal is to establish a reliable, scalable, and maintainable infrastructure for handling high-throughput transaction streams. Kafka enables fault-tolerant ingestion, while FastAPI serves RESTful endpoints for dashboard integration. PostgreSQL is used as the persistent storage backend, and the user-friendly Streamlit interface allows live monitoring of transaction KPIs. This setup also supports local development and testing through Docker and `.env` configuration, enabling seamless reproducibility.

---

## ğŸ§© System Components

### Microservices

* ğŸ§ª **Simulator**: Publishes synthetic transaction events to Kafka.
* ğŸ“¬ **Kafka Broker + ZooKeeper**: Facilitates distributed event streaming.
* ğŸ“¥ **Consumer**: Consumes Kafka messages and persists data to PostgreSQL.
* ğŸ§  **Backend API (FastAPI)**: Serves REST endpoints for transactions.
* ğŸ—ƒï¸ **PostgreSQL**: Stores structured transaction data.
* ğŸ“ˆ **Dashboard (Streamlit)**: Displays real-time analytics using backend API.

---

## ğŸ—ï¸ Architecture Diagram

```
    A[Simulator] -->|Send Events| B(Kafka Broker)
    B --> C[Consumer]
    C --> D[(PostgreSQL)]
    D --> E[FastAPI Backend]
    E --> F[Streamlit Dashboard]
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/malliknas/real-time-ecommerce-backend.git
cd real-time-ecommerce-backend
```

### 2ï¸âƒ£ Make Kafka Wait Script Executable

The simulator includes a script (`wait-for-kafka.sh`) that ensures Kafka is available before sending data.
You must mark it executable:

```bash
chmod +x simulator/wait-for-kafka.sh
```

### 2ï¸âƒ£ Start All Services

```bash
docker-compose up --build
```

This will start all components: Kafka, ZooKeeper, PostgreSQL, Simulator, Consumer, Backend API, and the Streamlit Dashboard.

> âœ… Prerequisites: Ensure Docker & Docker Compose are installed.

---

## ğŸ” Service Access & Test

| Component           | URL / Command                                                                    |
| ------------------- | -------------------------------------------------------------------------------- |
| FastAPI Docs        | [http://localhost:8000/docs](http://localhost:8000/docs)                         |
| Streamlit Dashboard | [http://localhost:8501](http://localhost:8501)                                   |
| PostgreSQL Shell    | `docker-compose exec db psql -U postgres -d financial_db`                        |
| Kafka Topic Check   | `docker-compose exec kafka kafka-topics.sh --list --bootstrap-server kafka:9092` |

---

## ğŸ“ Folder Structure

```bash
.
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Architecture/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â”œâ”€â”€ fastapi_swagger.png
â”‚   â”œâ”€â”€ streamlit_dashboard.png
â”‚   â””â”€â”€ kafka_output.png
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”œâ”€â”€ consumer/
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ dashboard.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”œâ”€â”€ simulator/
â”‚   â”œâ”€â”€ data_simulator.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ wait-for-kafka.sh
â”‚   â””â”€â”€ .env
```

---

## âš™ï¸ Environment Variables (`.env`)

```env
# PostgreSQL
POSTGRES_HOST=db
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=financial_db

# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
TRANSACTION_TOPIC=transactions_topic

# Backend API
API_BASE_URL=http://backend:8000
```

---

## ğŸ Local Run (Manual)

```bash
# Start core services first
docker-compose up kafka zookeeper db

# Then run each microservice locally

# Simulator
cd simulator
python data_simulator.py

# Consumer
cd consumer
python consumer.py

# Backend
cd backend
uvicorn app:app --host 0.0.0.0 --port 8000

# Dashboard
cd dashboard
streamlit run dashboard.py
```

---

## âœ… Validate Setup

```bash
# Kafka
docker-compose exec kafka kafka-console-consumer.sh \
  --bootstrap-server kafka:9092 \
  --topic transactions_topic \
  --from-beginning

# PostgreSQL Query
docker-compose exec db psql -U postgres -d financial_db
# Then run:
SELECT * FROM transactions LIMIT 10;

# Test API
curl http://localhost:8000/transactions/all
```

---

## ğŸ”’ Python Requirements Summary

Each service has its own `requirements.txt`, hereâ€™s a reference:

### `simulator/requirements.txt`

```
faker
kafka-python
python-dotenv
```

### `consumer/requirements.txt`

```
kafka-python
psycopg2-binary
python-dotenv
```

### `backend/requirements.txt`

```
fastapi
uvicorn
psycopg2-binary
python-dotenv
```

### `dashboard/requirements.txt`

```
streamlit
pandas
plotly
requests
python-dotenv
streamlit-autorefresh
```

---

## ğŸŒŸ Features

* ğŸ§  Kafka-based streaming pipeline
* ğŸ”„ Real-time transaction simulation and ingestion
* ğŸ” REST API endpoints for filtered data
* ğŸ§® PostgreSQL-backed persistent storage
* ğŸ“ˆ Live dashboard using Streamlit
* ğŸ³ Dockerized microservices architecture

---

## ğŸ“¸ Screenshots

> Screenshots for:
>
> * Architecture
> * Kafka consumer output
> * FastAPI Swagger UI
> * Streamlit Dashboard
>
> â¡ï¸ Added in `Architecture/`.

---

## ğŸ§‘â€ğŸ’» Author

**Malik Naseruddin**
IU University â€“ Task Submission
`AI Use Case Project`

---

## ğŸ“„ License

This project is open-source and free to use under the MIT License.

```
